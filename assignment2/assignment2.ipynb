{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ct_support_code import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.load('ct_data.npz')\n",
    "X_train = data['X_train']; X_val = data['X_val']; X_test = data['X_test']\n",
    "y_train = data['y_train']; y_val = data['y_val']; y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "y_train mean:  -9.13868774539957e-15\ny_val mean with standard error: -0.2160085093241599 ± 0.012903383410668334\ny_train 5785 samples mean with standard error: -0.44247687859693674 ± 0.01192627246273395\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "\"\\nWe see that y_train mean with the error bar for\\nonly 5785 samples is far away from zero (y_train mean).\\nAs number of samples in y_train is much larger than number of samples in y_val,\\nwe can't make our predictions of y_val mean just based on y_train mean \\nwith the standard error bar...better explanation..\\n\""
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 16
    }
   ],
   "source": [
    "# 1 a)\n",
    "y_train_mean = y_train.mean()\n",
    "print('y_train mean: ', y_train_mean)\n",
    "\n",
    "y_val_mean = y_val.mean()\n",
    "y_val_error = y_val.std()/np.sqrt(len(y_val))\n",
    "\n",
    "y_train_error = y_train[:5785].std()/np.sqrt(5785)\n",
    "print(f'y_val mean with standard error: {y_val_mean} ± {y_val_error}')\n",
    "print(f'y_train 5785 samples mean with standard error: {y_train[:5785].mean()} ± {y_train_error}')\n",
    "\n",
    "\"\"\"\n",
    "We see that y_train mean with the error bar for\n",
    "only 5785 samples is far away from zero (y_train mean).\n",
    "As number of samples in y_train is much larger than number of samples in y_val,\n",
    "we can't make our predictions of y_val mean just based on y_train mean \n",
    "with the standard error bar...better explanation..\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Indexes of constant columns: [59, 69, 179, 189, 351]\n",
      "Original indexes of duplicate columns [78, 79, 69, 179, 199, 188, 189, 351, 287, 359]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 1 b)\n",
    "\n",
    "remove_indexes = [i for i,col in enumerate(X_train.T) if np.all(col==col[0])]\n",
    "print('Indexes of constant columns:', remove_indexes)\n",
    "remove_indexes_2 = []\n",
    "for i, col1 in enumerate(X_train.T):\n",
    "    for j, col2 in enumerate(X_train.T[i+1:,:]):\n",
    "        if np.all(col1==col2):\n",
    "            remove_indexes_2.append(i+j+1)\n",
    "            break       \n",
    "print('Original indexes of duplicate columns', remove_indexes_2)\n",
    "remove_indexes = remove_indexes + remove_indexes_2\n",
    "X_train = np.delete(X_train, remove_indexes, axis=1)\n",
    "X_val = np.delete(X_val, remove_indexes, axis=1)\n",
    "X_test = np.delete(X_test, remove_indexes, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Training RMSE (fit_linreg):  0.3567669814948786\nTraining RMSE (fit_linreg_gradopt):  0.35675561493545876\nValidation RMSE (fit_linreg):  0.42292954946321326\nValidation RMSE (fit_linreg_gradopt):  0.42305590683687927\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "'\\nValues close to each other, but not exactly the same\\nfit_linreg_gradopt is better on training\\nAdd explanation..numerical reasons..different method?\\nval rmse smaller..?\\n'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 18
    }
   ],
   "source": [
    "# 2 \n",
    "def fit_linreg(X, yy, alpha):\n",
    "    K = X.shape[1]\n",
    "    N = X.shape[0]\n",
    "    X = np.column_stack((X,np.ones(N)))\n",
    "    X = np.row_stack((X,np.sqrt(alpha)*np.eye(K+1)))\n",
    "    y = np.concatenate((yy, np.zeros(K+1)))\n",
    "    w = np.linalg.lstsq(X,y,rcond=None)[0]\n",
    "    return w[:-1], w[-1]\n",
    "\n",
    "def rmse(ww, bb, XX, yy):\n",
    "    return np.sqrt(np.mean(((XX @ ww + bb) - yy)**2))\n",
    "\n",
    "alpha = 30\n",
    "train_w1, train_b1 = fit_linreg(X_train, y_train, alpha)\n",
    "train_w2, train_b2 = fit_linreg_gradopt(X_train, y_train, alpha)\n",
    "print('Training RMSE (fit_linreg): ', rmse(train_w1, train_b1, X_train, y_train))\n",
    "print('Training RMSE (fit_linreg_gradopt): ', rmse(train_w2, train_b2, X_train, y_train))\n",
    "print('Validation RMSE (fit_linreg): ', rmse(train_w1, train_b1, X_val, y_val))\n",
    "print('Validation RMSE (fit_linreg_gradopt): ', rmse(train_w2, train_b2, X_val, y_val))\n",
    "\n",
    "\"\"\"\n",
    "Values close to each other, but not exactly the same\n",
    "fit_linreg_gradopt is better on training\n",
    "Add explanation..numerical reasons..different method?\n",
    "val rmse smaller..?\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Training RMSE logreg transformation (fit_linreg_gradopt):  0.15441194427307056\nValidation RMSE logreg transformation (fit_linreg_gradopt):  0.2542485765839827\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "' Lower RMSE than normal linear regression '"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 19
    }
   ],
   "source": [
    "# 3\n",
    "K = 20 # number of thresholded classification problems to fit\n",
    "mx = np.max(y_train); mn = np.min(y_train); hh = (mx-mn)/(K+1)\n",
    "thresholds = np.linspace(mn+hh, mx-hh, num=K, endpoint=True)\n",
    "\n",
    "\n",
    "def fit_logreg_gradopt(X, yy, alpha):\n",
    "    D = X.shape[1]\n",
    "    args = (X, yy, alpha)\n",
    "    init = (np.zeros(D), np.array(0))\n",
    "    ww, bb = minimize_list(logreg_cost, init, args)\n",
    "    return ww, bb\n",
    "\n",
    "def logreg_forward(X, ww, bb):\n",
    "    aa = (X @ ww) + bb\n",
    "    return 1 / (1 + np.exp(-aa))\n",
    "\n",
    "N = X_train.shape[0]\n",
    "D = X_train.shape[1]\n",
    "WW = np.empty((D, K))\n",
    "BB = np.empty(K)\n",
    "LL = np.empty((N, K))\n",
    "for kk in range(K):\n",
    "    labels = y_train > thresholds[kk]\n",
    "    LL[:, kk] = labels\n",
    "    WW[:,kk], BB[kk] = fit_logreg_gradopt(X_train, labels, alpha)\n",
    "\n",
    "\n",
    "X_train_logreg = logreg_forward(X_train, WW, BB) \n",
    "X_val_logreg = logreg_forward(X_val, WW, BB) \n",
    "\n",
    "train_w_lr, train_b_lr = fit_linreg_gradopt(X_train_logreg, y_train, alpha)\n",
    "print('Training RMSE logreg transformation (fit_linreg_gradopt): ', \n",
    "      rmse(train_w_lr, train_b_lr, X_train_logreg, y_train))\n",
    "print('Validation RMSE logreg transformation (fit_linreg_gradopt): ', \n",
    "      rmse(train_w_lr, train_b_lr, X_val_logreg, y_val))\n",
    "\"\"\" Lower RMSE than normal linear regression \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "NN train RMSE with random initialization parameters:  0.13942065852907823\nNN val RMSE with random initialization parameters:  0.2684920995102662\nNN train RMSE with q3 initialization parameters:  0.13858869596445625\nNN val RMSE with q3 initialization parameters:  0.2706637451683212\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 4\n",
    "def fit_nn_gradopt(X, yy, alpha, K=20, init=None):\n",
    "    D = X.shape[1]\n",
    "    args = (X, yy, alpha)\n",
    "    if init==None:\n",
    "        init = (np.random.randn(K), np.array(0), np.random.randn(K, D), np.random.randn(K))\n",
    "    ww, bb, V, bk = minimize_list(nn_cost, init, args)\n",
    "    return ww, bb, V, bk\n",
    "\n",
    "def nn_rmse(params, XX, yy):\n",
    "    ww, bb, V, bk = params\n",
    "    \n",
    "    A = np.dot(XX, V.T) + bk[None,:] # N,K\n",
    "    P = 1 / (1 + np.exp(-A)) # N,K\n",
    "    F = np.dot(P, ww) + bb # N,\n",
    "    E =  np.sqrt(np.mean((F - yy)**2))\n",
    "    return E\n",
    "\n",
    "nn_rand_params = fit_nn_gradopt(X_train, y_train, 30)\n",
    "q3_params = (train_w_lr, train_b_lr, WW.T, BB)\n",
    "nn_q3_params = fit_nn_gradopt(X_train, y_train, 30, init=q3_params)\n",
    "#print(\"NN cost with random initialization parameters: \", nn_cost(nn_rand_params, X_train, yy=y_train, alpha=30)[0])\n",
    "#print(\"NN cost with q3 initialization parameters: \", nn_cost(nn_q3_params, X_train, yy=y_train, alpha=30)[0])\n",
    "print(\"NN train RMSE with random initialization parameters: \", nn_rmse(nn_rand_params, X_train, y_train))\n",
    "print(\"NN val RMSE with random initialization parameters: \", nn_rmse(nn_rand_params, X_val, y_val))\n",
    "print(\"NN train RMSE with q3 initialization parameters: \", nn_rmse(nn_q3_params, X_train, y_train))\n",
    "print(\"NN val RMSE with q3 initialization parameters: \", nn_rmse(nn_q3_params, X_val, y_val))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "0.2706637451683212"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 25
    }
   ],
   "source": [
    "# 5\n",
    "\n",
    "def train_nn_reg(X_train, y_train, X_val, y_val, alpha):\n",
    "    params = fit_nn_gradopt(X_train, y_train, alpha)\n",
    "    return nn_rmse(params, X_val, y_val)\n",
    "\n",
    "def prob_imp(mu, cov, yy, alphas, alpha):\n",
    "    a_idx = np.where(alphas==alpha)[0][0]\n",
    "    #pi = norm.cdf((mu[a_idx]-np.max(yy))/np.sqrt(cov[a_idx,a_idx]))\n",
    "    pi = (mu[a_idx] - np.max(yy)) / np.sqrt(cov[a_idx, a_idx])\n",
    "    return pi\n",
    "\n",
    "alphas = np.arange(0, 50, 0.02)\n",
    "idx = np.random.randint(0,len(alphas),size=3)\n",
    "y_train_gp = np.array([])\n",
    "train_alphas = alphas[idx]\n",
    "for alpha in train_alphas:\n",
    "    y_train_gp = np.append(y_train_gp, -np.log(train_nn_reg(X_train, y_train, X_val, y_val, alpha)))\n",
    "nn_rand_params = fit_nn_gradopt(X_train, y_train, 30)\n",
    "baseline = np.log(nn_rmse(nn_rand_params, X_val, y_val))\n",
    "#y_train_alpha = baseline + y_train_alpha\n",
    "test_alphas = np.delete(alphas, idx)\n",
    "\n",
    "for i in range(5):\n",
    "    mu, cov = gp_post_par(test_alphas, train_alphas, y_train_gp)\n",
    "\n",
    "    plt.plot(test_alphas, mu, '-k', linewidth=2)\n",
    "    std = np.sqrt(np.diag(cov))\n",
    "    plt.plot(test_alphas, mu + 2 * std, '--k', linewidth=2)\n",
    "    plt.plot(test_alphas, mu - 2 * std, '--k', linewidth=2)\n",
    "    plt.show()\n",
    "\n",
    "    best_alpha = test_alphas[0]\n",
    "    best_pi = - 1e100\n",
    "    for alpha in test_alphas:\n",
    "        pi = prob_imp(mu, cov, y_train_gp, test_alphas, alpha)\n",
    "        if pi > best_pi:\n",
    "            best_pi = pi\n",
    "            best_alpha = alpha\n",
    "    print(best_alpha, best_pi)\n",
    "    train_alphas = np.append(train_alphas, best_alpha)\n",
    "    test_alphas = np.delete(test_alphas, np.where(test_alphas==best_alpha))\n",
    "    y_train_gp = np.append(y_train_gp, - np.log(train_nn_reg(X_train, y_train, X_val, y_val, best_alpha)))\n",
    "    #y_train_alpha = np.append(y_train_alpha, baseline -np.log(train_nn_reg(X_train, y_train, X_val, y_val, best_alpha)))\n",
    "best_alpha = train_alphas[np.argmax(y_train_gp)]\n",
    "\n",
    "val_rmse = train_nn_reg(X_train, y_train, X_val, y_val, best_alpha)\n",
    "test_rmse = train_nn_reg(X_train, y_train, X_test, y_test, best_alpha)\n",
    "print(train_alphas)\n",
    "print('Best alpha: ', best_alpha)\n",
    "print('Val RMSE: ', val_rmse)\n",
    "print('Test RMSE: ', test_rmse)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21296/789015795.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;31m#                 np.random.randn(H,K), np.random.randn(H))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;31m# print(nn2_rmse(params,X_val, y_val))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m \u001b[0mnn_rand_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_nn2_gradopt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;31m#q3_params = (train_w_lr, train_b_lr, WW.T, BB)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21296/789015795.py\u001b[0m in \u001b[0;36mfit_nn2_gradopt\u001b[1;34m(X, yy, alpha, K, H, init)\u001b[0m\n\u001b[0;32m     52\u001b[0m         init = (np.random.randn(H), np.array(0), np.random.randn(K, D), np.random.randn(K),\n\u001b[0;32m     53\u001b[0m                 np.random.randn(H,K), np.random.randn(H))\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[0mww\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminimize_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn2_cost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mww\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitLab\\mlpr\\assignment2\\ct_support_code.py\u001b[0m in \u001b[0;36mminimize_list\u001b[1;34m(cost, init_list, args)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_bar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mvec_bar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams_wrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_bar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvec_bar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrap_cost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'L-BFGS-B'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\mlpr\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'l-bfgs-b'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m--> 624\u001b[1;33m                                 callback=callback, **options)\n\u001b[0m\u001b[0;32m    625\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tnc'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\mlpr\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    306\u001b[0m     sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n\u001b[0;32m    307\u001b[0m                                   \u001b[0mbounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_bounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                                   finite_diff_rel_step=finite_diff_rel_step)\n\u001b[0m\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[0mfunc_and_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun_and_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\mlpr\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;31m# calculation reduces overall function evaluations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     sf = ScalarFunction(fun, x0, args, grad, hess,\n\u001b[1;32m--> 262\u001b[1;33m                         finite_diff_rel_step, bounds, epsilon=epsilon)\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\mlpr\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;31m# Gradient evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\mlpr\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\mlpr\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\mlpr\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[1;31m# Overwriting results in undefined behaviour because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[1;31m# fun(self.x) will change self.x, with the two no longer linked.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\mlpr\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;34m\"\"\" returns the the function value \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\mlpr\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitLab\\mlpr\\assignment2\\ct_support_code.py\u001b[0m in \u001b[0;36mwrap_cost\u001b[1;34m(vec, *args)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'maxiter'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'disp'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0minit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munwrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams_wrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mwrap_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[0mE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_bar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mvec_bar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams_wrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_bar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable numpy.float64 object"
     ],
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable numpy.float64 object",
     "output_type": "error"
    }
   ],
   "source": [
    "#6\n",
    "\n",
    "\n",
    "def nn2_cost(params, X, yy=None, alpha=None):\n",
    "    \"\"\"NN_COST simple neural network cost function and gradients, or predictions\n",
    "\n",
    "           E, params_bar = nn_cost([ww, bb, V, bk], X, yy, alpha)\n",
    "                    pred = nn_cost([ww, bb, V, bk], X)\n",
    "\n",
    "     Cost function E can be minimized with minimize_list\n",
    "\n",
    "     Inputs:\n",
    "             params (ww, bb, V, bk), where:\n",
    "                    --------------------------------\n",
    "                        ww K,  hidden-output weights\n",
    "                        bb     scalar output bias\n",
    "                         V K,D hidden-input weights\n",
    "                        bk K,  hidden biases\n",
    "                    --------------------------------\n",
    "                  X N,D input design matrix\n",
    "                 yy N,  regression targets\n",
    "              alpha     scalar regularization for weights\n",
    "\n",
    "     Outputs:\n",
    "                     E  sum of squares error\n",
    "            params_bar  gradients wrt params, same format as params\n",
    "     OR\n",
    "               pred N,  predictions if only params and X are given as inputs\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb, V, bk, V2 ,b2 = params\n",
    "\n",
    "    # Forwards computation of cost\n",
    "    A = np.dot(X, V.T) + bk[None,:] # N,K\n",
    "    P = 1 / (1 + np.exp(-A)) # N,K\n",
    "    B = np.dot(P, V2.T) + b2[None,:] # \n",
    "    P2 = 1 / (1 + np.exp(-B)) # N,K\n",
    "    F = np.dot(P2, ww) + bb # N,\n",
    "    if yy is None:\n",
    "        # user wants prediction rather than training signal:\n",
    "        return F\n",
    "    res = F - yy # N,\n",
    "    E = np.dot(res, res) + alpha*(np.sum(V*V) + np.dot(ww,ww)) # 1x1\n",
    "\n",
    "    return E\n",
    "\n",
    "\n",
    "def fit_nn2_gradopt(X, yy, alpha, K=64, H = 32, init=None):\n",
    "    D = X.shape[1]\n",
    "    args = (X, yy, alpha)\n",
    "    if init==None:\n",
    "        init = (np.random.randn(H), np.array(0), np.random.randn(K, D), np.random.randn(K),\n",
    "                np.random.randn(H,K), np.random.randn(H))\n",
    "    ww, bb, V, bk, V2, b2 = minimize_list(nn2_cost, init, args)\n",
    "    return ww, bb, V, bk, V2, b2\n",
    "\n",
    "def nn2_rmse(params, XX, yy):\n",
    "    ww, bb, V, bk, V2, b2 = params\n",
    "    \n",
    "    A = np.dot(XX, V.T) + bk[None,:] # N,H\n",
    "    P = 1 / (1 + np.exp(-A)) # N,H\n",
    "    B = np.dot(P, V2.T) + b2[None,:] # \n",
    "    P2 = 1 / (1 + np.exp(-B)) # N,K\n",
    "    F = np.dot(P2, ww) + bb # N,\n",
    "    E =  np.sqrt(np.mean((F - yy)**2))\n",
    "    return E\n",
    "K =64\n",
    "H = 32\n",
    "# params = (np.random.randn(H), np.array(0), np.random.randn(K, D), np.random.randn(K),\n",
    "#                 np.random.randn(H,K), np.random.randn(H))\n",
    "# print(nn2_rmse(params,X_val, y_val))\n",
    "nn_rand_params = fit_nn2_gradopt(X_train, y_train, 30)\n",
    "\n",
    "#q3_params = (train_w_lr, train_b_lr, WW.T, BB)\n",
    "#nn_q3_params = fit_nn2gradopt(X_train, y_train, 30, init=q3_params)\n",
    "#print(\"NN cost with random initialization parameters: \", nn_cost(nn_rand_params, X_train, yy=y_train, alpha=30)[0])\n",
    "#print(\"NN cost with q3 initialization parameters: \", nn_cost(nn_q3_params, X_train, yy=y_train, alpha=30)[0])\n",
    "print(\"NN train RMSE with random initialization parameters: \", nn2_rmse(nn_rand_params, X_train, y_train))\n",
    "print(\"NN val RMSE with random initialization parameters: \", nn2_rmse(nn_rand_params, X_val, y_val))\n",
    "#print(\"NN train RMSE with q3 initialization parameters: \", nn2_rmse(nn_q3_params, X_train, y_train))\n",
    "#print(\"NN val RMSE with q3 initialization parameters: \", nn2_rmse(nn_q3_params, X_val, y_val))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "WW.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}